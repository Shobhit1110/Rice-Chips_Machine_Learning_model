# -*- coding: utf-8 -*-
"""Preprocessing.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18yHiW9kIjVRzjstdWo599VzshjEK3aFP
"""

!pip install mlend --upgrade

# Google Colab and Drive
from google.colab import drive

# Data Manipulation and Visualization
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Image Processing
from skimage import exposure, color, feature, io, measure
from skimage.color import rgb2hsv, rgb2gray
from skimage.feature import greycomatrix, ORB, graycoprops, greycoprops
from skimage.measure import shannon_entropy
from PIL import Image
import cv2
from skimage.feature import greycomatrix
from skimage.feature import local_binary_pattern
import numpy as np
from skimage import exposure
from skimage import color, morphology



# Machine Learning
import mlend
from mlend import download_yummy, yummy_load
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import LinearSVC, SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from imblearn.over_sampling import SMOTE
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import make_pipeline
from sklearn.metrics import roc_auc_score, matthews_corrcoef, roc_curve, auc, accuracy_score
from sklearn.preprocessing import LabelEncoder
from skimage.feature import greycomatrix,graycomatrix
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report
import xgboost as xgb
from scipy.stats import iqr


# Utilities
import os, sys, re, pickle, glob, urllib.request, zipfile
from scipy.stats import iqr
import IPython.display as ipd
from tqdm import tqdm
import librosa
drive.mount('/content/drive')

baseDir = download_yummy(save_to = '/content/drive/MyDrive/Data/MLEnd_Full')
baseDir
os.listdir(baseDir)

MLENDYD_df = pd.read_csv('/content/drive/MyDrive/Data/MLEnd_Full/yummy/MLEndYD_image_attributes_benchmark.csv')

sample_path = '/content/drive/MyDrive/Data/MLEnd_Full/yummy/MLEndYD_images/*.jpg'
files = glob.glob(sample_path)

"""
***The script classifies dishes in the MLENDYD_df DataFrame as 'rice', 'chips',  by examining keywords ('rice' or 'chips/fries/wedges') in the 'Ingredients' and 'Dish_name' columns.***"""

rice_rows = MLENDYD_df[MLENDYD_df['Dish_name'].str.contains('rice', case=False) | MLENDYD_df['Ingredients'].str.contains('rice', case=False)]

chips_rows = MLENDYD_df[(MLENDYD_df['Dish_name'].str.contains('chips|fries|wedges', case=False)) | (MLENDYD_df['Ingredients'].str.contains('chips|fries|wedges', case=False))]

rice_rows['Rice_Chips'] = 'rice'
chips_rows['Rice_Chips'] = 'chips'

"""***The code merges the Rice and Chips dataset.***"""

merged_dataset = pd.concat([rice_rows, chips_rows], ignore_index=True)

"""
***The code subsequently partitions the DataFrame into training and test sets, utilizing the 'Benchmark_A' column as the criterion for division.***"""

train_data = merged_dataset[merged_dataset['Benchmark_A'] == 'Train']
test_data = merged_dataset[merged_dataset['Benchmark_A'] == 'Test']

"""***Showing first five rows of train data***"""

display(train_data[:5])

"""***Showing first five rows of test data***"""

display(test_data[:5])

"""***Deleting the duplicates rows from train and test.***"""

train_data = train_data.drop_duplicates(subset='filename', keep='first')
test_data = test_data.drop_duplicates(subset='filename', keep='first')

"""**Showing the shape of train data after removing duplicates.**"""

train_data.shape

"""**Showing the shape of train data after removing duplicates.**"""

test_data.shape

"""**Shuffling the samples within the training and test datasets.**"""

train_data = train_data.sample(frac=1).reset_index(drop=True)
test_data = test_data.sample(frac=1).reset_index(drop=True)

"""***Showing first five rows of shuffled train data***"""

train_data[:5]

"""***Showing first five rows of shuffled test data***"""

test_data[:5]

"""
**Having obtained all 3250 samples, we now need to segregate the paths corresponding only to 'rice' and 'chips' in X_train_paths and X_test_paths.**"""

TrainSet, TestSet,Map = yummy_load(datadir_main=baseDir,train_test_split='Benchmark_A')

X_train_paths = TrainSet['X_paths']
X_test_paths  = TestSet['X_paths']

import os

def filter_paths_by_filenames(paths, data, filename_column='filename'):
    filename_to_path = {os.path.basename(path): path for path in paths}
    filtered_filenames = [filename for filename in data[filename_column].values if filename in filename_to_path]
    filtered_paths = [filename_to_path[filename] for filename in filtered_filenames]
    return filtered_paths

X_train_paths = filter_paths_by_filenames(X_train_paths, train_data)
X_test_paths = filter_paths_by_filenames(X_test_paths, test_data)

"""
***In this code, a LabelEncoder from scikit-learn is employed to convert categorical labels in the 'Rice_Chips' column of both the training ('train_data') and test ('test_data') datasets into numerical representations. The transformed labels are stored in 'Y_train' and 'Y_test', respectively.***"""

label_encoder = LabelEncoder()
Y_train = label_encoder.fit_transform(train_data['Rice_Chips'])
Y_test = label_encoder.fit_transform(test_data['Rice_Chips'])

"""***This code is to visually showcase a few examples of rice and chips images from the dataset.***"""

Chips_Img = np.array(X_train_paths)[Y_train==0]
Rice_Img = np.array(X_train_paths)[Y_train==1]

print('Rice')
plt.figure(figsize=(15,5))
for k,file in enumerate(Rice_Img[:5]):
  I = plt.imread(file)
  plt.subplot(1,5,k+1)
  plt.imshow(I)
  plt.axis('off')

plt.tight_layout()
plt.show()

print('Chips')
plt.figure(figsize=(15,5))
for k,file in enumerate(Chips_Img[:5]):
  I = plt.imread(file)
  plt.subplot(1,5,k+1)
  plt.imshow(I)
  plt.axis('off')

plt.tight_layout()
plt.show()